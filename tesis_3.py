# -*- coding: utf-8 -*-
"""tesis 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VkrtuMEgOJjC9KK9kusuLeQyp3LXS0SW
"""

# @title Instalaciones
!npm install -g localtunnel
!pip install flask flask-ngrok
!pip install pefile
!pip install pyngrok

# Commented out IPython magic to ensure Python compatibility.
# @title Importaciones
# %matplotlib inline
import io
import os
import csv
import pefile
import joblib
import psutil
import subprocess
from pyngrok import ngrok
from sklearn import metrics
from collections import Counter
import numpy as np # álgebra lineal
from flask import Flask, request, jsonify
import seaborn as sns # gráficos avanzados
import pandas as pd # procesamiento de datos
import matplotlib.pyplot as plt # gáficos básicos
from sklearn.pipeline import Pipeline # sikit learn pipline
from sklearn import model_selection,linear_model,decomposition
from sklearn.pipeline import make_pipeline # sikit learn pipline
from sklearn.model_selection import train_test_split # división de datos
from sklearn.preprocessing import StandardScaler # normalización de datos
from sklearn.model_selection import GridSearchCV, ShuffleSplit # cross validation
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_score,recall_score,f1_score,roc_auc_score, roc_curve, make_scorer # evaluación de métricas

# @title Cargar la data
from google.colab import files
uploaded = files.upload()

# @title Visualizar la data
df = pd.read_csv(io.BytesIO(uploaded['df_new_15.1.csv']))
df.head()

# @title Visualizar y eliminar si existen datos duplicados
df [df.duplicated() == True]
# eliminar las filas duplicadas
df1 = df.drop_duplicates()

# @title Visualizar y eliminar si existen valores nulos
nulls = df.isna().sum() # contar valores nulos en cada columna
df_nulls = pd.DataFrame(nulls) # convertir el resultado en un dataframe
df_nulls.transpose() # transponer el marco de datos e imprimir el resultado

# @title Dividimos los datos en entrenamiento y prueba
from sklearn.model_selection import train_test_split

X = df1.drop('Malware', axis=1)
y = df1['Malware']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=0, stratify=y)

print("Ejemplos usados Malware entrenar: ", len(X_train))
print("Ejemplos usados para test: ", len(X_test))
# Mostramos los datos de la columna 'Malware' para el conjunto de prueba
y_test.value_counts()

# @title Ajuste de Parametros con GridSearchCV para LightGBM
!pip install lightgbm
import lightgbm as lgb
from lightgbm import LGBMClassifier
#Iniciar el modelo
model = LGBMClassifier(random_state=0)

# @title Operaciones en orden
operations = [('model', model)]
#Definir el pipeline
pipe = Pipeline(operations)

# @title Entrenamiento del pipeline
import warnings
warnings.filterwarnings('ignore')
pipe.fit(X_train, y_train)
pipe_pred = pipe.predict(X_test)
tn, fp, fn, tp = confusion_matrix(y_test, pipe_pred).ravel()
print('Leyenda »', 'tn: ', tn, 'fp: ', fp, 'fn: ', fn, 'tp: ', tp)
plt.figure(figsize=(5,2))
plt.title("Matriz de Confusión del Pipeline:", fontsize=16)
sns.heatmap(confusion_matrix(y_test, pipe_pred), annot=True, cmap='Greys', fmt='.0f')
plt.show();

# @title Establecer el parámetro del grid
param_grid = {
    'num_leaves': [31, 62, 127],
    'learning_rate': [0.01, 0.05, 0.1],
    'min_child_samples': [10, 20, 30],
    'max_depth': [3, 5, 7],
}
scoring = {
    'sensitivity': make_scorer(recall_score),
    'specificity': make_scorer(recall_score,pos_label=0)
}

# @title Poniendo todo junto
full_cv_classifier = GridSearchCV(model, param_grid, cv=5, scoring=scoring,
                                  refit='sensitivity', verbose=3)
# Entrenamos el Pipeline
full_cv_classifier.fit(X_train, y_train.values.ravel())

# @title Mejores learning_rate, max_depth y random_state
full_cv_classifier.best_estimator_

# @title Mejores parámetros del modelo
full_cv_classifier.best_estimator_.get_params()

# @title Iniciar y configurar las operaciones
modelLGBM = LGBMClassifier(
    boosting_type='gbdt',        # Mantener el tipo de boosting
    class_weight=None,           # Mantener el peso de clase
    colsample_bytree=1.0,       # Puede probar 0.8 si quieres reducir la complejidad
    importance_type='split',     # Mantener el tipo de importancia
    learning_rate=0.1,           # Mantener la tasa de aprendizaje
    max_depth=7,                 # Mantener la profundidad máxima
    min_child_samples=10,        # Mantener el número mínimo de muestras por hoja
    min_child_weight=0.001,      # Mantener el peso mínimo por hoja
    min_split_gain=0.0,          # Mantener la ganancia mínima de división
    n_estimators=100,            # Mantener el número de estimadores
    num_leaves=127,              # Mantener el número de hojas
    random_state=0,              # Mantener la semilla aleatoria
    reg_alpha=0.0,               # Puede probar 0.1 para regularización L1
    reg_lambda=0.0,              # Puede probar 0.1 para regularización L2
    subsample=1.0,               # Puede probar 0.8 para usar una fracción de los datos
    subsample_for_bin=200000,    # Mantener el número de muestras para el bin
    subsample_freq=0             # Mantener la frecuencia de subsample
)
operations = [('modelLGBM', modelLGBM)]

# @title Configurar el pipeline
pipe = Pipeline(operations)

# @title Entrenamiento del pipeline
pipe.fit(X_train, y_train)

# @title Predicción con el conjunto de prueba
pipe_pred = pipe.predict(X_test)

# @title Resultado de las métricas para LGBM despues del GridSearchCV
import warnings
warnings.filterwarnings('ignore')
gbc_auc=roc_auc_score(y_test, modelLGBM.predict_proba(X_test)[:,1])
tn, fp, fn, tp = confusion_matrix(y_test, pipe_pred).ravel()
print('tn : ', tn)
print('fp : ', fp)
print('fn : ', fn)
print('tp : ', tp)
a=accuracy_score(y_test, modelLGBM.predict(X_test))
print("(accuracy_score) =  {}".format(a))
p=precision_score(y_test, modelLGBM.predict(X_test))
print("(precision_score) =  {}".format(p))
r=recall_score(y_test, modelLGBM.predict(X_test))
print("(recall_score) =  {}".format(r))
f1=f1_score(y_test, modelLGBM.predict(X_test))
print("(f1_score) =  {}".format(f1))
print("(auc_score) =  {}".format(gbc_auc))
specificity = tn / (tn+fp)
print('specificity : ', specificity)
sensitivity = tp / (tp+fn)
print('sensitivity : ', sensitivity)
G_Mean= np.sqrt(sensitivity * specificity)
print('G-Mean  : ', G_Mean)

# @title Calculando la tasa de verdaderos positivos y tasa de verdaderos negativos para todos los umbrales de la clasificación
probs = modelLGBM.predict_proba(X_test)
preds = probs[:,1]
fpr, tpr, threshold = metrics.roc_curve(y_test, preds)
roc_auc = metrics.auc(fpr, tpr)

# method: plt
plt.title('Receiver Operating Characteristic')
plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
plt.legend(loc = 'lower right')
plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0, 1])
plt.ylim([0, 1])
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.show()

# @title Crear la aplicación Flask
app = Flask(__name__)
# Configuración
app.config['MAX_CONTENT_LENGTH'] = 20 * 1024 * 1024  # Limitar el tamaño máximo del archivo a 1MB
app.config['UPLOAD_FOLDER'] = 'uploads'
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)  # Crear la carpeta de subida si no existe

# @title Cargar el modelo LightGBM
model = pipe

# @title Ajustar ngrok
ngrok.set_auth_token("Generar token en ngrok") #Generar token para continuar

# @title Iniciar ngrok
public_url = ngrok.connect(5000)
print(" * ngrok URL:", public_url)

# @title Configurar el metodo POST de la la app
@app.route('/upload', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return jsonify({'error': 'No se ha seleccionado ningún archivo'}), 400
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No se ha seleccionado ningún archivo'}), 400
    if not file.filename.endswith('.csv'):
        return jsonify({'error': 'Solo se permiten archivos CSV'}), 400

    # Verificar el tamaño del archivo (1MB = 20 * 1024 * 1024 bytes)
    if file.content_length > 20 * 1024 * 1024:
        return jsonify({'error': 'El archivo debe ser menor a 2MB'}), 400

    # Guardar el archivo
    file_path = os.path.join(app.config['UPLOAD_FOLDER'], file.filename)
    file.save(file_path)

    # Cargar las características desde el archivo CSV
    features_df = pd.read_csv(file_path)

   # Realizar la predicción
    prediction = model.predict(features_df)

    # Imprimir la predicción en la consola
    print("Predicción:", prediction.tolist())

    # Convertir la predicción a "malware" o "no malware"
    results = ["malware" if pred == 1 else "no malware" for pred in prediction]

    # Devolver la predicción como respuesta
    return jsonify({'prediction': results}), 200

# @title Poner la app en linea para predicciones
if __name__ == '__main__':
    app.run(port=5000)

#@title Guarda el modelo entrenado
import joblib
joblib.dump(model, 'lgbm_model.pkl')